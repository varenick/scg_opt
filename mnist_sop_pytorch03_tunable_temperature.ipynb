{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.0.post4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as distr\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (15, 8)\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "USE_CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cuda_wrapper(tensor, use_cuda=USE_CUDA):\n",
    "    if use_cuda:\n",
    "        return tensor.cuda()\n",
    "    else:\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "kwargs = {}\n",
    "batch_size = 24\n",
    "test_batch_size = 24\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        './data', train=True, download=False,\n",
    "        transform=transforms.Compose(\n",
    "            #[transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "            [transforms.ToTensor()]\n",
    "        )\n",
    "    ), batch_size=batch_size, shuffle=True, **kwargs\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        './data', train=False, \n",
    "        transform=transforms.Compose(\n",
    "            #[transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "            [transforms.ToTensor()]\n",
    "        )\n",
    "    ), batch_size=test_batch_size, shuffle=True, **kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SigmoidBeliefNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=None, num_blocks=1, hidden_dim=200, nonlinear_blocks=False):\n",
    "        super(SigmoidBeliefNetwork, self).__init__()\n",
    "        \n",
    "        if output_dim is None:\n",
    "            output_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_blocks = num_blocks\n",
    "        \n",
    "        layer_input_dim = input_dim\n",
    "        layer_output_dim = hidden_dim\n",
    "            \n",
    "        self.dense_layers = nn.ModuleList()\n",
    "        self.nonlinearities = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(num_blocks):\n",
    "            self.dense_layers.append(nn.ModuleList())\n",
    "            self.nonlinearities.append(nn.ModuleList())\n",
    "            if nonlinear_blocks:\n",
    "                for _ in range(3):\n",
    "                    self.dense_layers[-1].append(nn.Linear(layer_input_dim, layer_output_dim))\n",
    "                    layer_input_dim = layer_output_dim\n",
    "                    layer_output_dim = hidden_dim\n",
    "                for _ in range(2):\n",
    "                    self.nonlinearities[-1].append(nn.Tanh())\n",
    "                self.nonlinearities[-1].append(nn.Sigmoid())\n",
    "            else:\n",
    "                self.dense_layers[-1].append(nn.Linear(layer_input_dim, layer_output_dim))\n",
    "                layer_input_dim = layer_output_dim\n",
    "                layer_output_dim = hidden_dim\n",
    "                self.nonlinearities[-1].append(nn.Sigmoid())\n",
    "                    \n",
    "        self.output_layer = nn.Linear(layer_input_dim, output_dim)\n",
    "        \n",
    "        self.eps = 1.0e-6\n",
    "        \n",
    "    def _soft_forward(self, X, sigmoid_temp, start_block_idx):\n",
    "        soft_output = X\n",
    "        for i in range(start_block_idx, self.num_blocks):\n",
    "            for dense_layer, nonlinearity in zip(self.dense_layers[i], self.nonlinearities[i]):\n",
    "                soft_output = nonlinearity(dense_layer(soft_output))\n",
    "            prob = torch.clamp(soft_output, min=self.eps, max=1.-self.eps)\n",
    "            \n",
    "            u = autograd.Variable(\n",
    "                cuda_wrapper(torch.Tensor(prob.size()).uniform_()), requires_grad=False\n",
    "            )\n",
    "\n",
    "            z = torch.log(prob * u + self.eps) - torch.log((1 - prob) * (1 - u) + self.eps)\n",
    "            soft_output = torch.sigmoid(z / sigmoid_temp)\n",
    "            \n",
    "        return self.output_layer(soft_output)\n",
    "            \n",
    "    def forward(self, X, sigmoid_temp=1.0):\n",
    "        self.sample_log_probs = []\n",
    "        final_soft_outputs = []\n",
    "        final_cond_soft_outputs = []\n",
    "\n",
    "        hard_output = X\n",
    "        for i in range(self.num_blocks):\n",
    "            for dense_layer, nonlinearity in zip(self.dense_layers[i], self.nonlinearities[i]):\n",
    "                hard_output = nonlinearity(dense_layer(hard_output))\n",
    "            prob = torch.clamp(hard_output, min=self.eps, max=1.-self.eps)\n",
    "            \n",
    "            u = autograd.Variable(\n",
    "                cuda_wrapper(torch.Tensor(prob.size()).uniform_()), requires_grad=False\n",
    "            )\n",
    "            v = autograd.Variable(\n",
    "                cuda_wrapper(torch.Tensor(prob.size()).uniform_()), requires_grad=False\n",
    "            )\n",
    "\n",
    "            z = torch.log(prob * u + self.eps) - torch.log((1 - prob) * (1 - u) + self.eps)\n",
    "            b = (z > 0)\n",
    "            v = v * (1 - prob) * (1 - b.float()) + (v * prob + (1 - prob)) * b.float() \n",
    "            z_cond = torch.log(prob * v + self.eps) - torch.log((1 - prob) * (1 - v) + self.eps)\n",
    "\n",
    "            self.sample_log_probs.append(distr.Bernoulli(prob).log_prob(b))\n",
    "            \n",
    "            hard_output = b.float()\n",
    "            soft_output = torch.sigmoid(z / sigmoid_temp)\n",
    "            cond_soft_output = torch.sigmoid(z_cond / sigmoid_temp)\n",
    "            \n",
    "            final_soft_outputs.append(self._soft_forward(soft_output, sigmoid_temp, start_block_idx=i+1))\n",
    "            final_cond_soft_outputs.append(self._soft_forward(cond_soft_output, sigmoid_temp, start_block_idx=i+1))\n",
    "        \n",
    "        final_hard_output = self.output_layer(hard_output)\n",
    "        return final_hard_output, final_soft_outputs, final_cond_soft_outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = train_loader.dataset.train_data.size()\n",
    "input_dim = input_dim[1] * input_dim[2] // 2\n",
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "rebar_0.1\n",
      "Epoch 1 of 200\n",
      "loss: 115.71\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0002, eta_b: 0.9999\n",
      "layer 2:\n",
      "eta_w: 1.0004, eta_b: 1.0000\n",
      "layer 3:\n",
      "eta_w: 1.0003, eta_b: 1.0003\n",
      "sigmoid_temp: 0.3837\n",
      "elapsed time: 44.0s\n",
      "\n",
      "Epoch 2 of 200\n",
      "loss: 109.08\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0012, eta_b: 1.0006\n",
      "layer 2:\n",
      "eta_w: 1.0006, eta_b: 1.0007\n",
      "layer 3:\n",
      "eta_w: 1.0002, eta_b: 1.0002\n",
      "sigmoid_temp: 0.5225\n",
      "elapsed time: 43.8s\n",
      "\n",
      "Epoch 3 of 200\n",
      "loss: 109.02\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0011, eta_b: 1.0011\n",
      "layer 2:\n",
      "eta_w: 1.0003, eta_b: 1.0007\n",
      "layer 3:\n",
      "eta_w: 1.0001, eta_b: 1.0001\n",
      "sigmoid_temp: 0.5468\n",
      "elapsed time: 44.2s\n",
      "\n",
      "Epoch 4 of 200\n",
      "loss: 109.01\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0015, eta_b: 1.0022\n",
      "layer 2:\n",
      "eta_w: 1.0006, eta_b: 1.0009\n",
      "layer 3:\n",
      "eta_w: 1.0000, eta_b: 1.0000\n",
      "sigmoid_temp: 0.5280\n",
      "elapsed time: 44.2s\n",
      "\n",
      "Epoch 5 of 200\n",
      "loss: 109.00\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0016, eta_b: 1.0012\n",
      "layer 2:\n",
      "eta_w: 1.0009, eta_b: 1.0009\n",
      "layer 3:\n",
      "eta_w: 0.9999, eta_b: 0.9999\n",
      "sigmoid_temp: 0.5044\n",
      "elapsed time: 43.5s\n",
      "\n",
      "Epoch 6 of 200\n",
      "loss: 108.99\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0012, eta_b: 1.0014\n",
      "layer 2:\n",
      "eta_w: 1.0007, eta_b: 1.0010\n",
      "layer 3:\n",
      "eta_w: 1.0000, eta_b: 1.0000\n",
      "sigmoid_temp: 0.5082\n",
      "elapsed time: 43.2s\n",
      "\n",
      "Epoch 7 of 200\n",
      "loss: 108.99\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0164, eta_b: 1.0118\n",
      "layer 2:\n",
      "eta_w: 1.0001, eta_b: 1.0050\n",
      "layer 3:\n",
      "eta_w: 0.9999, eta_b: 1.0000\n",
      "sigmoid_temp: 0.5267\n",
      "elapsed time: 43.8s\n",
      "\n",
      "Epoch 8 of 200\n",
      "loss: 108.99\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0040, eta_b: 1.0026\n",
      "layer 2:\n",
      "eta_w: 1.0000, eta_b: 1.0008\n",
      "layer 3:\n",
      "eta_w: 0.9999, eta_b: 0.9999\n",
      "sigmoid_temp: 0.5274\n",
      "elapsed time: 43.8s\n",
      "\n",
      "Epoch 9 of 200\n",
      "loss: 108.99\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0093, eta_b: 1.0176\n",
      "layer 2:\n",
      "eta_w: 1.0034, eta_b: 1.0050\n",
      "layer 3:\n",
      "eta_w: 1.0002, eta_b: 1.0001\n",
      "sigmoid_temp: 0.4187\n",
      "elapsed time: 43.9s\n",
      "\n",
      "Epoch 10 of 200\n",
      "loss: 108.99\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0109, eta_b: 1.0120\n",
      "layer 2:\n",
      "eta_w: 1.0264, eta_b: 0.9847\n",
      "layer 3:\n",
      "eta_w: 1.0001, eta_b: 1.0000\n",
      "sigmoid_temp: 0.5004\n",
      "elapsed time: 44.1s\n",
      "\n",
      "Epoch 11 of 200\n",
      "loss: 108.99\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 0.9946, eta_b: 0.9886\n",
      "layer 2:\n",
      "eta_w: 0.9805, eta_b: 0.9853\n",
      "layer 3:\n",
      "eta_w: 1.0000, eta_b: 0.9998\n",
      "sigmoid_temp: 0.4781\n",
      "elapsed time: 44.3s\n",
      "\n",
      "Epoch 12 of 200\n",
      "loss: 108.98\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0005, eta_b: 1.0005\n",
      "layer 2:\n",
      "eta_w: 1.0016, eta_b: 1.0025\n",
      "layer 3:\n",
      "eta_w: 1.0001, eta_b: 1.0001\n",
      "sigmoid_temp: 0.5002\n",
      "elapsed time: 44.1s\n",
      "\n",
      "Epoch 13 of 200\n",
      "loss: 108.98\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 0.9846, eta_b: 0.9885\n",
      "layer 2:\n",
      "eta_w: 1.0019, eta_b: 1.0025\n",
      "layer 3:\n",
      "eta_w: 1.0000, eta_b: 1.0000\n",
      "sigmoid_temp: 0.5105\n",
      "elapsed time: 44.4s\n",
      "\n",
      "Epoch 14 of 200\n",
      "loss: 108.98\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 0.9944, eta_b: 0.9948\n",
      "layer 2:\n",
      "eta_w: 1.0019, eta_b: 1.0025\n",
      "layer 3:\n",
      "eta_w: 1.0001, eta_b: 1.0001\n",
      "sigmoid_temp: 0.5065\n",
      "elapsed time: 44.0s\n",
      "\n",
      "Epoch 15 of 200\n",
      "loss: 108.98\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0016, eta_b: 1.0008\n",
      "layer 2:\n",
      "eta_w: 1.0018, eta_b: 1.0021\n",
      "layer 3:\n",
      "eta_w: 1.0001, eta_b: 1.0001\n",
      "sigmoid_temp: 0.5152\n",
      "elapsed time: 43.9s\n",
      "\n",
      "Epoch 16 of 200\n",
      "loss: 108.98\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0029, eta_b: 1.0010\n",
      "layer 2:\n",
      "eta_w: 1.0019, eta_b: 1.0020\n",
      "layer 3:\n",
      "eta_w: 1.0000, eta_b: 1.0000\n",
      "sigmoid_temp: 0.5473\n",
      "elapsed time: 44.1s\n",
      "\n",
      "Epoch 17 of 200\n",
      "loss: 108.98\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0053, eta_b: 1.0021\n",
      "layer 2:\n",
      "eta_w: 1.0019, eta_b: 1.0018\n",
      "layer 3:\n",
      "eta_w: 1.0000, eta_b: 1.0000\n",
      "sigmoid_temp: 0.5681\n",
      "elapsed time: 43.9s\n",
      "\n",
      "Epoch 18 of 200\n",
      "loss: 108.98\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0035, eta_b: 1.0026\n",
      "layer 2:\n",
      "eta_w: 1.0017, eta_b: 1.0016\n",
      "layer 3:\n",
      "eta_w: 0.9999, eta_b: 0.9999\n",
      "sigmoid_temp: 0.5665\n",
      "elapsed time: 44.1s\n",
      "\n",
      "Epoch 19 of 200\n",
      "loss: 108.98\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0027, eta_b: 1.0018\n",
      "layer 2:\n",
      "eta_w: 1.0016, eta_b: 1.0016\n",
      "layer 3:\n",
      "eta_w: 0.9998, eta_b: 0.9998\n",
      "sigmoid_temp: 0.5376\n",
      "elapsed time: 44.2s\n",
      "\n",
      "Epoch 20 of 200\n",
      "loss: 108.98\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0042, eta_b: 1.0061\n",
      "layer 2:\n",
      "eta_w: 1.0016, eta_b: 1.0015\n",
      "layer 3:\n",
      "eta_w: 1.0000, eta_b: 1.0000\n",
      "sigmoid_temp: 0.5509\n",
      "elapsed time: 43.7s\n",
      "\n",
      "Epoch 21 of 200\n",
      "loss: 108.98\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0078, eta_b: 1.0079\n",
      "layer 2:\n",
      "eta_w: 1.0016, eta_b: 1.0014\n",
      "layer 3:\n",
      "eta_w: 1.0001, eta_b: 1.0001\n",
      "sigmoid_temp: 0.5556\n",
      "elapsed time: 44.1s\n",
      "\n",
      "Epoch 22 of 200\n",
      "loss: 108.98\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0031, eta_b: 1.0045\n",
      "layer 2:\n",
      "eta_w: 1.0016, eta_b: 1.0015\n",
      "layer 3:\n",
      "eta_w: 1.0003, eta_b: 1.0003\n",
      "sigmoid_temp: 0.5786\n",
      "elapsed time: 44.1s\n",
      "\n",
      "Epoch 23 of 200\n",
      "loss: 108.98\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 0.9998, eta_b: 1.0016\n",
      "layer 2:\n",
      "eta_w: 1.0014, eta_b: 1.0007\n",
      "layer 3:\n",
      "eta_w: 1.0002, eta_b: 1.0002\n",
      "sigmoid_temp: 0.5859\n",
      "elapsed time: 43.9s\n",
      "\n",
      "Epoch 24 of 200\n",
      "loss: 108.97\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 0.9639, eta_b: 0.9588\n",
      "layer 2:\n",
      "eta_w: 0.9977, eta_b: 0.9733\n",
      "layer 3:\n",
      "eta_w: 1.0004, eta_b: 1.0000\n",
      "sigmoid_temp: 0.6011\n",
      "elapsed time: 43.6s\n",
      "\n",
      "Epoch 25 of 200\n",
      "loss: 108.97\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 0.9997, eta_b: 1.0021\n",
      "layer 2:\n",
      "eta_w: 1.0008, eta_b: 1.0010\n",
      "layer 3:\n",
      "eta_w: 1.0006, eta_b: 1.0006\n",
      "sigmoid_temp: 0.6042\n",
      "elapsed time: 43.9s\n",
      "\n",
      "Epoch 26 of 200\n",
      "loss: 108.97\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 0.9989, eta_b: 1.0004\n",
      "layer 2:\n",
      "eta_w: 1.0010, eta_b: 1.0011\n",
      "layer 3:\n",
      "eta_w: 1.0006, eta_b: 1.0006\n",
      "sigmoid_temp: 0.5983\n",
      "elapsed time: 43.9s\n",
      "\n",
      "Epoch 27 of 200\n",
      "loss: 108.97\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 0.9987, eta_b: 1.0003\n",
      "layer 2:\n",
      "eta_w: 1.0007, eta_b: 1.0007\n",
      "layer 3:\n",
      "eta_w: 1.0004, eta_b: 1.0003\n",
      "sigmoid_temp: 0.6154\n",
      "elapsed time: 44.4s\n",
      "\n",
      "Epoch 28 of 200\n",
      "loss: 108.97\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0000, eta_b: 0.9977\n",
      "layer 2:\n",
      "eta_w: 1.0004, eta_b: 0.9980\n",
      "layer 3:\n",
      "eta_w: 1.0003, eta_b: 1.0003\n",
      "sigmoid_temp: 0.6269\n",
      "elapsed time: 44.0s\n",
      "\n",
      "Epoch 29 of 200\n",
      "loss: 108.97\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0155, eta_b: 1.0197\n",
      "layer 2:\n",
      "eta_w: 1.0104, eta_b: 1.0164\n",
      "layer 3:\n",
      "eta_w: 1.0006, eta_b: 1.0002\n",
      "sigmoid_temp: 0.3490\n",
      "elapsed time: 44.7s\n",
      "\n",
      "Epoch 30 of 200\n",
      "loss: 108.97\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0005, eta_b: 1.0005\n",
      "layer 2:\n",
      "eta_w: 1.0034, eta_b: 1.0038\n",
      "layer 3:\n",
      "eta_w: 1.0005, eta_b: 1.0005\n",
      "sigmoid_temp: 0.6108\n",
      "elapsed time: 44.0s\n",
      "\n",
      "Epoch 31 of 200\n",
      "loss: 108.97\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 0.9999, eta_b: 1.0002\n",
      "layer 2:\n",
      "eta_w: 1.0005, eta_b: 1.0009\n",
      "layer 3:\n",
      "eta_w: 1.0009, eta_b: 1.0009\n",
      "sigmoid_temp: 0.6041\n",
      "elapsed time: 43.4s\n",
      "\n",
      "Epoch 32 of 200\n",
      "loss: 108.97\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 0.9996, eta_b: 1.0016\n",
      "layer 2:\n",
      "eta_w: 0.9997, eta_b: 1.0006\n",
      "layer 3:\n",
      "eta_w: 1.0004, eta_b: 1.0003\n",
      "sigmoid_temp: 0.6206\n",
      "elapsed time: 43.9s\n",
      "\n",
      "Epoch 33 of 200\n",
      "loss: 107.85\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0065, eta_b: 1.0044\n",
      "layer 2:\n",
      "eta_w: 1.0010, eta_b: 0.9996\n",
      "layer 3:\n",
      "eta_w: 0.9953, eta_b: 0.9954\n",
      "sigmoid_temp: 1.7463\n",
      "elapsed time: 44.3s\n",
      "\n",
      "Epoch 34 of 200\n",
      "loss: 102.98\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0062, eta_b: 1.0073\n",
      "layer 2:\n",
      "eta_w: 1.0084, eta_b: 1.0085\n",
      "layer 3:\n",
      "eta_w: 1.0016, eta_b: 1.0015\n",
      "sigmoid_temp: 0.4038\n",
      "elapsed time: 43.9s\n",
      "\n",
      "Epoch 35 of 200\n",
      "loss: 100.66\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0017, eta_b: 1.0028\n",
      "layer 2:\n",
      "eta_w: 1.0039, eta_b: 1.0036\n",
      "layer 3:\n",
      "eta_w: 1.0006, eta_b: 1.0006\n",
      "sigmoid_temp: 0.3225\n",
      "elapsed time: 43.3s\n",
      "\n",
      "Epoch 36 of 200\n",
      "loss: 99.18\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0088, eta_b: 1.0098\n",
      "layer 2:\n",
      "eta_w: 1.0094, eta_b: 1.0091\n",
      "layer 3:\n",
      "eta_w: 1.0009, eta_b: 1.0008\n",
      "sigmoid_temp: 0.4132\n",
      "elapsed time: 43.2s\n",
      "\n",
      "Epoch 37 of 200\n",
      "loss: 97.00\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0038, eta_b: 1.0036\n",
      "layer 2:\n",
      "eta_w: 1.0097, eta_b: 1.0095\n",
      "layer 3:\n",
      "eta_w: 1.0002, eta_b: 1.0003\n",
      "sigmoid_temp: 0.4564\n",
      "elapsed time: 43.6s\n",
      "\n",
      "Epoch 38 of 200\n",
      "loss: 94.29\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0035, eta_b: 1.0033\n",
      "layer 2:\n",
      "eta_w: 1.0083, eta_b: 1.0080\n",
      "layer 3:\n",
      "eta_w: 1.0010, eta_b: 1.0011\n",
      "sigmoid_temp: 0.3455\n",
      "elapsed time: 44.2s\n",
      "\n",
      "Epoch 39 of 200\n",
      "loss: 92.32\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0014, eta_b: 1.0027\n",
      "layer 2:\n",
      "eta_w: 1.0047, eta_b: 1.0048\n",
      "layer 3:\n",
      "eta_w: 1.0017, eta_b: 1.0016\n",
      "sigmoid_temp: 0.4101\n",
      "elapsed time: 43.1s\n",
      "\n",
      "Epoch 40 of 200\n",
      "loss: 90.50\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0036, eta_b: 1.0028\n",
      "layer 2:\n",
      "eta_w: 1.0042, eta_b: 1.0041\n",
      "layer 3:\n",
      "eta_w: 1.0011, eta_b: 1.0011\n",
      "sigmoid_temp: 0.5565\n",
      "elapsed time: 42.4s\n",
      "\n",
      "Epoch 41 of 200\n",
      "loss: 88.06\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0067, eta_b: 1.0065\n",
      "layer 2:\n",
      "eta_w: 1.0061, eta_b: 1.0058\n",
      "layer 3:\n",
      "eta_w: 1.0019, eta_b: 1.0019\n",
      "sigmoid_temp: 0.4167\n",
      "elapsed time: 43.3s\n",
      "\n",
      "Epoch 42 of 200\n",
      "loss: 85.73\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0068, eta_b: 1.0053\n",
      "layer 2:\n",
      "eta_w: 1.0039, eta_b: 1.0034\n",
      "layer 3:\n",
      "eta_w: 1.0012, eta_b: 1.0012\n",
      "sigmoid_temp: 0.4152\n",
      "elapsed time: 44.1s\n",
      "\n",
      "Epoch 43 of 200\n",
      "loss: 83.98\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0079, eta_b: 1.0086\n",
      "layer 2:\n",
      "eta_w: 1.0058, eta_b: 1.0055\n",
      "layer 3:\n",
      "eta_w: 1.0028, eta_b: 1.0028\n",
      "sigmoid_temp: 0.5130\n",
      "elapsed time: 44.0s\n",
      "\n",
      "Epoch 44 of 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 82.63\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0127, eta_b: 1.0133\n",
      "layer 2:\n",
      "eta_w: 1.0085, eta_b: 1.0083\n",
      "layer 3:\n",
      "eta_w: 1.0020, eta_b: 1.0017\n",
      "sigmoid_temp: 0.6222\n",
      "elapsed time: 44.4s\n",
      "\n",
      "Epoch 45 of 200\n",
      "loss: 81.44\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0129, eta_b: 1.0130\n",
      "layer 2:\n",
      "eta_w: 1.0103, eta_b: 1.0103\n",
      "layer 3:\n",
      "eta_w: 1.0041, eta_b: 1.0040\n",
      "sigmoid_temp: 0.6165\n",
      "elapsed time: 44.4s\n",
      "\n",
      "Epoch 46 of 200\n",
      "loss: 80.18\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0114, eta_b: 1.0101\n",
      "layer 2:\n",
      "eta_w: 1.0093, eta_b: 1.0090\n",
      "layer 3:\n",
      "eta_w: 1.0040, eta_b: 1.0039\n",
      "sigmoid_temp: 0.6726\n",
      "elapsed time: 45.3s\n",
      "\n",
      "Epoch 47 of 200\n",
      "loss: 78.95\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0096, eta_b: 1.0083\n",
      "layer 2:\n",
      "eta_w: 1.0098, eta_b: 1.0092\n",
      "layer 3:\n",
      "eta_w: 1.0039, eta_b: 1.0038\n",
      "sigmoid_temp: 0.6108\n",
      "elapsed time: 45.3s\n",
      "\n",
      "Epoch 48 of 200\n",
      "loss: 77.78\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0102, eta_b: 1.0092\n",
      "layer 2:\n",
      "eta_w: 1.0098, eta_b: 1.0096\n",
      "layer 3:\n",
      "eta_w: 1.0048, eta_b: 1.0046\n",
      "sigmoid_temp: 0.7551\n",
      "elapsed time: 44.3s\n",
      "\n",
      "Epoch 49 of 200\n",
      "loss: 76.71\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0144, eta_b: 1.0152\n",
      "layer 2:\n",
      "eta_w: 1.0118, eta_b: 1.0122\n",
      "layer 3:\n",
      "eta_w: 1.0050, eta_b: 1.0049\n",
      "sigmoid_temp: 0.7138\n",
      "elapsed time: 43.8s\n",
      "\n",
      "Epoch 50 of 200\n",
      "loss: 75.75\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0102, eta_b: 1.0108\n",
      "layer 2:\n",
      "eta_w: 1.0100, eta_b: 1.0103\n",
      "layer 3:\n",
      "eta_w: 1.0034, eta_b: 1.0032\n",
      "sigmoid_temp: 0.4193\n",
      "elapsed time: 43.9s\n",
      "\n",
      "Epoch 51 of 200\n",
      "loss: 74.92\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0137, eta_b: 1.0158\n",
      "layer 2:\n",
      "eta_w: 1.0120, eta_b: 1.0123\n",
      "layer 3:\n",
      "eta_w: 1.0042, eta_b: 1.0045\n",
      "sigmoid_temp: 0.6676\n",
      "elapsed time: 45.1s\n",
      "\n",
      "Epoch 52 of 200\n",
      "loss: 74.27\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0100, eta_b: 1.0111\n",
      "layer 2:\n",
      "eta_w: 1.0081, eta_b: 1.0084\n",
      "layer 3:\n",
      "eta_w: 1.0034, eta_b: 1.0037\n",
      "sigmoid_temp: 0.8128\n",
      "elapsed time: 43.7s\n",
      "\n",
      "Epoch 53 of 200\n",
      "loss: 73.67\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0107, eta_b: 1.0128\n",
      "layer 2:\n",
      "eta_w: 1.0097, eta_b: 1.0104\n",
      "layer 3:\n",
      "eta_w: 1.0045, eta_b: 1.0045\n",
      "sigmoid_temp: 0.6540\n",
      "elapsed time: 43.8s\n",
      "\n",
      "Epoch 54 of 200\n",
      "loss: 73.14\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0102, eta_b: 1.0122\n",
      "layer 2:\n",
      "eta_w: 1.0079, eta_b: 1.0085\n",
      "layer 3:\n",
      "eta_w: 1.0024, eta_b: 1.0018\n",
      "sigmoid_temp: 0.7506\n",
      "elapsed time: 43.8s\n",
      "\n",
      "Epoch 55 of 200\n",
      "loss: 72.68\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0120, eta_b: 1.0133\n",
      "layer 2:\n",
      "eta_w: 1.0093, eta_b: 1.0097\n",
      "layer 3:\n",
      "eta_w: 1.0040, eta_b: 1.0037\n",
      "sigmoid_temp: 0.7534\n",
      "elapsed time: 44.0s\n",
      "\n",
      "Epoch 56 of 200\n",
      "loss: 72.25\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0128, eta_b: 1.0134\n",
      "layer 2:\n",
      "eta_w: 1.0105, eta_b: 1.0107\n",
      "layer 3:\n",
      "eta_w: 1.0049, eta_b: 1.0049\n",
      "sigmoid_temp: 0.7525\n",
      "elapsed time: 43.8s\n",
      "\n",
      "Epoch 57 of 200\n",
      "loss: 71.85\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0122, eta_b: 1.0143\n",
      "layer 2:\n",
      "eta_w: 1.0099, eta_b: 1.0104\n",
      "layer 3:\n",
      "eta_w: 1.0044, eta_b: 1.0044\n",
      "sigmoid_temp: 0.7653\n",
      "elapsed time: 43.8s\n",
      "\n",
      "Epoch 58 of 200\n",
      "loss: 71.49\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0107, eta_b: 1.0117\n",
      "layer 2:\n",
      "eta_w: 1.0085, eta_b: 1.0087\n",
      "layer 3:\n",
      "eta_w: 1.0038, eta_b: 1.0035\n",
      "sigmoid_temp: 0.6563\n",
      "elapsed time: 44.4s\n",
      "\n",
      "Epoch 59 of 200\n",
      "loss: 71.13\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0115, eta_b: 1.0121\n",
      "layer 2:\n",
      "eta_w: 1.0092, eta_b: 1.0097\n",
      "layer 3:\n",
      "eta_w: 1.0032, eta_b: 1.0029\n",
      "sigmoid_temp: 0.6715\n",
      "elapsed time: 44.0s\n",
      "\n",
      "Epoch 60 of 200\n",
      "loss: 70.81\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0100, eta_b: 1.0126\n",
      "layer 2:\n",
      "eta_w: 1.0093, eta_b: 1.0101\n",
      "layer 3:\n",
      "eta_w: 1.0038, eta_b: 1.0035\n",
      "sigmoid_temp: 0.7733\n",
      "elapsed time: 44.5s\n",
      "\n",
      "Epoch 61 of 200\n",
      "loss: 70.52\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0074, eta_b: 1.0101\n",
      "layer 2:\n",
      "eta_w: 1.0060, eta_b: 1.0068\n",
      "layer 3:\n",
      "eta_w: 1.0018, eta_b: 1.0016\n",
      "sigmoid_temp: 0.6847\n",
      "elapsed time: 43.8s\n",
      "\n",
      "Epoch 62 of 200\n",
      "loss: 70.23\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0074, eta_b: 1.0097\n",
      "layer 2:\n",
      "eta_w: 1.0071, eta_b: 1.0081\n",
      "layer 3:\n",
      "eta_w: 1.0019, eta_b: 1.0017\n",
      "sigmoid_temp: 0.7943\n",
      "elapsed time: 45.3s\n",
      "\n",
      "Epoch 63 of 200\n",
      "loss: 69.99\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0069, eta_b: 1.0091\n",
      "layer 2:\n",
      "eta_w: 1.0055, eta_b: 1.0060\n",
      "layer 3:\n",
      "eta_w: 1.0015, eta_b: 1.0013\n",
      "sigmoid_temp: 0.7281\n",
      "elapsed time: 45.4s\n",
      "\n",
      "Epoch 64 of 200\n",
      "loss: 69.72\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0104, eta_b: 1.0101\n",
      "layer 2:\n",
      "eta_w: 1.0073, eta_b: 1.0074\n",
      "layer 3:\n",
      "eta_w: 1.0032, eta_b: 1.0028\n",
      "sigmoid_temp: 0.5690\n",
      "elapsed time: 43.7s\n",
      "\n",
      "Epoch 65 of 200\n",
      "loss: 69.44\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0060, eta_b: 1.0076\n",
      "layer 2:\n",
      "eta_w: 1.0051, eta_b: 1.0055\n",
      "layer 3:\n",
      "eta_w: 1.0010, eta_b: 1.0009\n",
      "sigmoid_temp: 0.7675\n",
      "elapsed time: 44.0s\n",
      "\n",
      "Epoch 66 of 200\n",
      "loss: 69.22\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0063, eta_b: 1.0074\n",
      "layer 2:\n",
      "eta_w: 1.0052, eta_b: 1.0056\n",
      "layer 3:\n",
      "eta_w: 1.0016, eta_b: 1.0012\n",
      "sigmoid_temp: 0.7714\n",
      "elapsed time: 43.9s\n",
      "\n",
      "Epoch 67 of 200\n",
      "loss: 68.99\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0087, eta_b: 1.0096\n",
      "layer 2:\n",
      "eta_w: 1.0065, eta_b: 1.0068\n",
      "layer 3:\n",
      "eta_w: 1.0027, eta_b: 1.0024\n",
      "sigmoid_temp: 0.6551\n",
      "elapsed time: 44.2s\n",
      "\n",
      "Epoch 68 of 200\n",
      "loss: 68.76\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0087, eta_b: 1.0091\n",
      "layer 2:\n",
      "eta_w: 1.0070, eta_b: 1.0072\n",
      "layer 3:\n",
      "eta_w: 1.0027, eta_b: 1.0023\n",
      "sigmoid_temp: 0.6183\n",
      "elapsed time: 44.1s\n",
      "\n",
      "Epoch 69 of 200\n",
      "loss: 68.54\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0073, eta_b: 1.0074\n",
      "layer 2:\n",
      "eta_w: 1.0063, eta_b: 1.0066\n",
      "layer 3:\n",
      "eta_w: 1.0032, eta_b: 1.0032\n",
      "sigmoid_temp: 0.6500\n",
      "elapsed time: 44.1s\n",
      "\n",
      "Epoch 70 of 200\n",
      "loss: 68.35\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0078, eta_b: 1.0068\n",
      "layer 2:\n",
      "eta_w: 1.0066, eta_b: 1.0064\n",
      "layer 3:\n",
      "eta_w: 1.0035, eta_b: 1.0031\n",
      "sigmoid_temp: 0.7319\n",
      "elapsed time: 44.6s\n",
      "\n",
      "Epoch 71 of 200\n",
      "loss: 68.16\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0091, eta_b: 1.0079\n",
      "layer 2:\n",
      "eta_w: 1.0076, eta_b: 1.0074\n",
      "layer 3:\n",
      "eta_w: 1.0035, eta_b: 1.0034\n",
      "sigmoid_temp: 0.6499\n",
      "elapsed time: 44.3s\n",
      "\n",
      "Epoch 72 of 200\n",
      "loss: 68.01\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0085, eta_b: 1.0080\n",
      "layer 2:\n",
      "eta_w: 1.0069, eta_b: 1.0068\n",
      "layer 3:\n",
      "eta_w: 1.0040, eta_b: 1.0038\n",
      "sigmoid_temp: 0.7716\n",
      "elapsed time: 44.3s\n",
      "\n",
      "Epoch 73 of 200\n",
      "loss: 67.83\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0092, eta_b: 1.0102\n",
      "layer 2:\n",
      "eta_w: 1.0078, eta_b: 1.0083\n",
      "layer 3:\n",
      "eta_w: 1.0041, eta_b: 1.0039\n",
      "sigmoid_temp: 0.7901\n",
      "elapsed time: 44.1s\n",
      "\n",
      "Epoch 74 of 200\n",
      "loss: 67.67\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0104, eta_b: 1.0110\n",
      "layer 2:\n",
      "eta_w: 1.0079, eta_b: 1.0082\n",
      "layer 3:\n",
      "eta_w: 1.0046, eta_b: 1.0044\n",
      "sigmoid_temp: 0.8021\n",
      "elapsed time: 44.2s\n",
      "\n",
      "Epoch 75 of 200\n",
      "loss: 67.52\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0109, eta_b: 1.0120\n",
      "layer 2:\n",
      "eta_w: 1.0091, eta_b: 1.0098\n",
      "layer 3:\n",
      "eta_w: 1.0045, eta_b: 1.0043\n",
      "sigmoid_temp: 0.8008\n",
      "elapsed time: 45.4s\n",
      "\n",
      "Epoch 76 of 200\n",
      "loss: 67.37\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0100, eta_b: 1.0115\n",
      "layer 2:\n",
      "eta_w: 1.0087, eta_b: 1.0094\n",
      "layer 3:\n",
      "eta_w: 1.0052, eta_b: 1.0050\n",
      "sigmoid_temp: 0.7188\n",
      "elapsed time: 44.1s\n",
      "\n",
      "Epoch 77 of 200\n",
      "loss: 67.20\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0070, eta_b: 1.0082\n",
      "layer 2:\n",
      "eta_w: 1.0063, eta_b: 1.0067\n",
      "layer 3:\n",
      "eta_w: 1.0040, eta_b: 1.0037\n",
      "sigmoid_temp: 0.6483\n",
      "elapsed time: 45.8s\n",
      "\n",
      "Epoch 78 of 200\n",
      "loss: 67.06\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0109, eta_b: 1.0128\n",
      "layer 2:\n",
      "eta_w: 1.0090, eta_b: 1.0096\n",
      "layer 3:\n",
      "eta_w: 1.0060, eta_b: 1.0063\n",
      "sigmoid_temp: 0.8123\n",
      "elapsed time: 43.9s\n",
      "\n",
      "Epoch 79 of 200\n",
      "loss: 66.93\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0098, eta_b: 1.0116\n",
      "layer 2:\n",
      "eta_w: 1.0083, eta_b: 1.0091\n",
      "layer 3:\n",
      "eta_w: 1.0053, eta_b: 1.0054\n",
      "sigmoid_temp: 0.8211\n",
      "elapsed time: 45.1s\n",
      "\n",
      "Epoch 80 of 200\n",
      "loss: 66.81\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0095, eta_b: 1.0113\n",
      "layer 2:\n",
      "eta_w: 1.0081, eta_b: 1.0089\n",
      "layer 3:\n",
      "eta_w: 1.0052, eta_b: 1.0052\n",
      "sigmoid_temp: 0.7715\n",
      "elapsed time: 45.2s\n",
      "\n",
      "Epoch 81 of 200\n",
      "loss: 66.69\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0092, eta_b: 1.0118\n",
      "layer 2:\n",
      "eta_w: 1.0083, eta_b: 1.0091\n",
      "layer 3:\n",
      "eta_w: 1.0064, eta_b: 1.0061\n",
      "sigmoid_temp: 0.8138\n",
      "elapsed time: 45.3s\n",
      "\n",
      "Epoch 82 of 200\n",
      "loss: 66.57\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0107, eta_b: 1.0125\n",
      "layer 2:\n",
      "eta_w: 1.0089, eta_b: 1.0096\n",
      "layer 3:\n",
      "eta_w: 1.0064, eta_b: 1.0064\n",
      "sigmoid_temp: 0.8394\n",
      "elapsed time: 45.5s\n",
      "\n",
      "Epoch 83 of 200\n",
      "loss: 66.45\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0074, eta_b: 1.0075\n",
      "layer 2:\n",
      "eta_w: 1.0064, eta_b: 1.0066\n",
      "layer 3:\n",
      "eta_w: 1.0057, eta_b: 1.0057\n",
      "sigmoid_temp: 0.8756\n",
      "elapsed time: 44.4s\n",
      "\n",
      "Epoch 84 of 200\n",
      "loss: 66.35\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0079, eta_b: 1.0085\n",
      "layer 2:\n",
      "eta_w: 1.0082, eta_b: 1.0085\n",
      "layer 3:\n",
      "eta_w: 1.0059, eta_b: 1.0058\n",
      "sigmoid_temp: 0.8105\n",
      "elapsed time: 44.6s\n",
      "\n",
      "Epoch 85 of 200\n",
      "loss: 66.24\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0085, eta_b: 1.0085\n",
      "layer 2:\n",
      "eta_w: 1.0074, eta_b: 1.0077\n",
      "layer 3:\n",
      "eta_w: 1.0055, eta_b: 1.0054\n",
      "sigmoid_temp: 0.8000\n",
      "elapsed time: 44.2s\n",
      "\n",
      "Epoch 86 of 200\n",
      "loss: 66.12\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0082, eta_b: 1.0089\n",
      "layer 2:\n",
      "eta_w: 1.0060, eta_b: 1.0063\n",
      "layer 3:\n",
      "eta_w: 1.0048, eta_b: 1.0048\n",
      "sigmoid_temp: 0.7717\n",
      "elapsed time: 44.2s\n",
      "\n",
      "Epoch 87 of 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 66.01\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0072, eta_b: 1.0060\n",
      "layer 2:\n",
      "eta_w: 1.0062, eta_b: 1.0063\n",
      "layer 3:\n",
      "eta_w: 1.0047, eta_b: 1.0045\n",
      "sigmoid_temp: 0.7651\n",
      "elapsed time: 44.2s\n",
      "\n",
      "Epoch 88 of 200\n",
      "loss: 65.90\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0084, eta_b: 1.0076\n",
      "layer 2:\n",
      "eta_w: 1.0069, eta_b: 1.0067\n",
      "layer 3:\n",
      "eta_w: 1.0054, eta_b: 1.0055\n",
      "sigmoid_temp: 0.7813\n",
      "elapsed time: 43.8s\n",
      "\n",
      "Epoch 89 of 200\n",
      "loss: 65.80\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0079, eta_b: 1.0071\n",
      "layer 2:\n",
      "eta_w: 1.0074, eta_b: 1.0074\n",
      "layer 3:\n",
      "eta_w: 1.0064, eta_b: 1.0060\n",
      "sigmoid_temp: 0.7531\n",
      "elapsed time: 44.3s\n",
      "\n",
      "Epoch 90 of 200\n",
      "loss: 65.70\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0064, eta_b: 1.0062\n",
      "layer 2:\n",
      "eta_w: 1.0060, eta_b: 1.0065\n",
      "layer 3:\n",
      "eta_w: 1.0049, eta_b: 1.0046\n",
      "sigmoid_temp: 0.7741\n",
      "elapsed time: 44.3s\n",
      "\n",
      "Epoch 91 of 200\n",
      "loss: 65.61\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0078, eta_b: 1.0072\n",
      "layer 2:\n",
      "eta_w: 1.0063, eta_b: 1.0063\n",
      "layer 3:\n",
      "eta_w: 1.0062, eta_b: 1.0057\n",
      "sigmoid_temp: 0.7521\n",
      "elapsed time: 44.6s\n",
      "\n",
      "Epoch 92 of 200\n",
      "loss: 65.51\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0077, eta_b: 1.0081\n",
      "layer 2:\n",
      "eta_w: 1.0074, eta_b: 1.0078\n",
      "layer 3:\n",
      "eta_w: 1.0068, eta_b: 1.0066\n",
      "sigmoid_temp: 0.7689\n",
      "elapsed time: 44.3s\n",
      "\n",
      "Epoch 93 of 200\n",
      "loss: 65.42\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0073, eta_b: 1.0073\n",
      "layer 2:\n",
      "eta_w: 1.0065, eta_b: 1.0067\n",
      "layer 3:\n",
      "eta_w: 1.0061, eta_b: 1.0060\n",
      "sigmoid_temp: 0.7619\n",
      "elapsed time: 44.1s\n",
      "\n",
      "Epoch 94 of 200\n",
      "loss: 65.32\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0074, eta_b: 1.0078\n",
      "layer 2:\n",
      "eta_w: 1.0066, eta_b: 1.0070\n",
      "layer 3:\n",
      "eta_w: 1.0065, eta_b: 1.0062\n",
      "sigmoid_temp: 0.7135\n",
      "elapsed time: 44.6s\n",
      "\n",
      "Epoch 95 of 200\n",
      "loss: 65.22\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0071, eta_b: 1.0079\n",
      "layer 2:\n",
      "eta_w: 1.0076, eta_b: 1.0084\n",
      "layer 3:\n",
      "eta_w: 1.0068, eta_b: 1.0070\n",
      "sigmoid_temp: 0.8082\n",
      "elapsed time: 44.2s\n",
      "\n",
      "Epoch 96 of 200\n",
      "loss: 65.16\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0073, eta_b: 1.0074\n",
      "layer 2:\n",
      "eta_w: 1.0068, eta_b: 1.0075\n",
      "layer 3:\n",
      "eta_w: 1.0062, eta_b: 1.0059\n",
      "sigmoid_temp: 0.7945\n",
      "elapsed time: 44.3s\n",
      "\n",
      "Epoch 97 of 200\n",
      "loss: 65.06\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0066, eta_b: 1.0074\n",
      "layer 2:\n",
      "eta_w: 1.0065, eta_b: 1.0071\n",
      "layer 3:\n",
      "eta_w: 1.0059, eta_b: 1.0059\n",
      "sigmoid_temp: 0.7878\n",
      "elapsed time: 44.0s\n",
      "\n",
      "Epoch 98 of 200\n",
      "loss: 64.99\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0074, eta_b: 1.0070\n",
      "layer 2:\n",
      "eta_w: 1.0069, eta_b: 1.0072\n",
      "layer 3:\n",
      "eta_w: 1.0064, eta_b: 1.0063\n",
      "sigmoid_temp: 0.7416\n",
      "elapsed time: 44.7s\n",
      "\n",
      "Epoch 99 of 200\n",
      "loss: 64.91\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0071, eta_b: 1.0084\n",
      "layer 2:\n",
      "eta_w: 1.0072, eta_b: 1.0080\n",
      "layer 3:\n",
      "eta_w: 1.0062, eta_b: 1.0061\n",
      "sigmoid_temp: 0.8356\n",
      "elapsed time: 44.8s\n",
      "\n",
      "Epoch 100 of 200\n",
      "loss: 64.83\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0065, eta_b: 1.0077\n",
      "layer 2:\n",
      "eta_w: 1.0069, eta_b: 1.0077\n",
      "layer 3:\n",
      "eta_w: 1.0065, eta_b: 1.0063\n",
      "sigmoid_temp: 0.8133\n",
      "elapsed time: 45.0s\n",
      "\n",
      "Epoch 101 of 200\n",
      "loss: 64.75\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0073, eta_b: 1.0093\n",
      "layer 2:\n",
      "eta_w: 1.0068, eta_b: 1.0077\n",
      "layer 3:\n",
      "eta_w: 1.0059, eta_b: 1.0054\n",
      "sigmoid_temp: 0.7664\n",
      "elapsed time: 45.7s\n",
      "\n",
      "Epoch 102 of 200\n",
      "loss: 64.67\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0048, eta_b: 1.0065\n",
      "layer 2:\n",
      "eta_w: 1.0054, eta_b: 1.0063\n",
      "layer 3:\n",
      "eta_w: 1.0056, eta_b: 1.0054\n",
      "sigmoid_temp: 0.7526\n",
      "elapsed time: 46.1s\n",
      "\n",
      "Epoch 103 of 200\n",
      "loss: 64.60\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0065, eta_b: 1.0078\n",
      "layer 2:\n",
      "eta_w: 1.0068, eta_b: 1.0075\n",
      "layer 3:\n",
      "eta_w: 1.0058, eta_b: 1.0053\n",
      "sigmoid_temp: 0.8427\n",
      "elapsed time: 45.4s\n",
      "\n",
      "Epoch 104 of 200\n",
      "loss: 64.54\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0036, eta_b: 1.0039\n",
      "layer 2:\n",
      "eta_w: 1.0044, eta_b: 1.0049\n",
      "layer 3:\n",
      "eta_w: 1.0040, eta_b: 1.0037\n",
      "sigmoid_temp: 0.6059\n",
      "elapsed time: 46.4s\n",
      "\n",
      "Epoch 105 of 200\n",
      "loss: 64.43\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0067, eta_b: 1.0067\n",
      "layer 2:\n",
      "eta_w: 1.0071, eta_b: 1.0075\n",
      "layer 3:\n",
      "eta_w: 1.0064, eta_b: 1.0060\n",
      "sigmoid_temp: 0.7176\n",
      "elapsed time: 45.4s\n",
      "\n",
      "Epoch 106 of 200\n",
      "loss: 64.36\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0050, eta_b: 1.0055\n",
      "layer 2:\n",
      "eta_w: 1.0059, eta_b: 1.0066\n",
      "layer 3:\n",
      "eta_w: 1.0051, eta_b: 1.0050\n",
      "sigmoid_temp: 0.7752\n",
      "elapsed time: 45.7s\n",
      "\n",
      "Epoch 107 of 200\n",
      "loss: 64.30\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0047, eta_b: 1.0048\n",
      "layer 2:\n",
      "eta_w: 1.0059, eta_b: 1.0064\n",
      "layer 3:\n",
      "eta_w: 1.0051, eta_b: 1.0047\n",
      "sigmoid_temp: 0.7537\n",
      "elapsed time: 45.2s\n",
      "\n",
      "Epoch 108 of 200\n",
      "loss: 64.23\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0032, eta_b: 1.0038\n",
      "layer 2:\n",
      "eta_w: 1.0044, eta_b: 1.0047\n",
      "layer 3:\n",
      "eta_w: 1.0046, eta_b: 1.0043\n",
      "sigmoid_temp: 0.7726\n",
      "elapsed time: 45.3s\n",
      "\n",
      "Epoch 109 of 200\n",
      "loss: 64.17\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0035, eta_b: 1.0042\n",
      "layer 2:\n",
      "eta_w: 1.0051, eta_b: 1.0058\n",
      "layer 3:\n",
      "eta_w: 1.0037, eta_b: 1.0032\n",
      "sigmoid_temp: 0.7289\n",
      "elapsed time: 45.8s\n",
      "\n",
      "Epoch 110 of 200\n",
      "loss: 64.09\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0036, eta_b: 1.0045\n",
      "layer 2:\n",
      "eta_w: 1.0044, eta_b: 1.0052\n",
      "layer 3:\n",
      "eta_w: 1.0037, eta_b: 1.0035\n",
      "sigmoid_temp: 0.7292\n",
      "elapsed time: 46.4s\n",
      "\n",
      "Epoch 111 of 200\n",
      "loss: 64.02\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0033, eta_b: 1.0041\n",
      "layer 2:\n",
      "eta_w: 1.0048, eta_b: 1.0054\n",
      "layer 3:\n",
      "eta_w: 1.0045, eta_b: 1.0042\n",
      "sigmoid_temp: 0.7342\n",
      "elapsed time: 45.1s\n",
      "\n",
      "Epoch 112 of 200\n",
      "loss: 63.96\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0040, eta_b: 1.0048\n",
      "layer 2:\n",
      "eta_w: 1.0053, eta_b: 1.0059\n",
      "layer 3:\n",
      "eta_w: 1.0042, eta_b: 1.0040\n",
      "sigmoid_temp: 0.7894\n",
      "elapsed time: 45.2s\n",
      "\n",
      "Epoch 113 of 200\n",
      "loss: 63.90\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0046, eta_b: 1.0060\n",
      "layer 2:\n",
      "eta_w: 1.0063, eta_b: 1.0071\n",
      "layer 3:\n",
      "eta_w: 1.0047, eta_b: 1.0044\n",
      "sigmoid_temp: 0.7430\n",
      "elapsed time: 45.4s\n",
      "\n",
      "Epoch 114 of 200\n",
      "loss: 63.85\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0061, eta_b: 1.0074\n",
      "layer 2:\n",
      "eta_w: 1.0065, eta_b: 1.0073\n",
      "layer 3:\n",
      "eta_w: 1.0050, eta_b: 1.0050\n",
      "sigmoid_temp: 0.7885\n",
      "elapsed time: 45.6s\n",
      "\n",
      "Epoch 115 of 200\n",
      "loss: 63.78\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0054, eta_b: 1.0072\n",
      "layer 2:\n",
      "eta_w: 1.0061, eta_b: 1.0071\n",
      "layer 3:\n",
      "eta_w: 1.0041, eta_b: 1.0039\n",
      "sigmoid_temp: 0.7431\n",
      "elapsed time: 46.3s\n",
      "\n",
      "Epoch 116 of 200\n",
      "loss: 63.71\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0064, eta_b: 1.0082\n",
      "layer 2:\n",
      "eta_w: 1.0067, eta_b: 1.0078\n",
      "layer 3:\n",
      "eta_w: 1.0047, eta_b: 1.0045\n",
      "sigmoid_temp: 0.7660\n",
      "elapsed time: 46.4s\n",
      "\n",
      "Epoch 117 of 200\n",
      "loss: 63.66\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0064, eta_b: 1.0080\n",
      "layer 2:\n",
      "eta_w: 1.0064, eta_b: 1.0075\n",
      "layer 3:\n",
      "eta_w: 1.0051, eta_b: 1.0049\n",
      "sigmoid_temp: 0.7545\n",
      "elapsed time: 46.7s\n",
      "\n",
      "Epoch 118 of 200\n",
      "loss: 63.60\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0049, eta_b: 1.0053\n",
      "layer 2:\n",
      "eta_w: 1.0051, eta_b: 1.0056\n",
      "layer 3:\n",
      "eta_w: 1.0046, eta_b: 1.0043\n",
      "sigmoid_temp: 0.7371\n",
      "elapsed time: 46.6s\n",
      "\n",
      "Epoch 119 of 200\n",
      "loss: 63.54\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0049, eta_b: 1.0053\n",
      "layer 2:\n",
      "eta_w: 1.0060, eta_b: 1.0065\n",
      "layer 3:\n",
      "eta_w: 1.0051, eta_b: 1.0048\n",
      "sigmoid_temp: 0.8106\n",
      "elapsed time: 46.5s\n",
      "\n",
      "Epoch 120 of 200\n",
      "loss: 63.50\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0036, eta_b: 1.0042\n",
      "layer 2:\n",
      "eta_w: 1.0039, eta_b: 1.0044\n",
      "layer 3:\n",
      "eta_w: 1.0036, eta_b: 1.0032\n",
      "sigmoid_temp: 0.7462\n",
      "elapsed time: 46.9s\n",
      "\n",
      "Epoch 121 of 200\n",
      "loss: 63.43\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0051, eta_b: 1.0056\n",
      "layer 2:\n",
      "eta_w: 1.0050, eta_b: 1.0055\n",
      "layer 3:\n",
      "eta_w: 1.0038, eta_b: 1.0035\n",
      "sigmoid_temp: 0.7457\n",
      "elapsed time: 46.5s\n",
      "\n",
      "Epoch 122 of 200\n",
      "loss: 63.38\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0056, eta_b: 1.0060\n",
      "layer 2:\n",
      "eta_w: 1.0050, eta_b: 1.0056\n",
      "layer 3:\n",
      "eta_w: 1.0041, eta_b: 1.0037\n",
      "sigmoid_temp: 0.7446\n",
      "elapsed time: 47.2s\n",
      "\n",
      "Epoch 123 of 200\n",
      "loss: 63.32\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0042, eta_b: 1.0060\n",
      "layer 2:\n",
      "eta_w: 1.0036, eta_b: 1.0044\n",
      "layer 3:\n",
      "eta_w: 1.0020, eta_b: 1.0016\n",
      "sigmoid_temp: 0.6633\n",
      "elapsed time: 47.0s\n",
      "\n",
      "Epoch 124 of 200\n",
      "loss: 63.25\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0041, eta_b: 1.0039\n",
      "layer 2:\n",
      "eta_w: 1.0038, eta_b: 1.0044\n",
      "layer 3:\n",
      "eta_w: 1.0019, eta_b: 1.0014\n",
      "sigmoid_temp: 0.6757\n",
      "elapsed time: 46.4s\n",
      "\n",
      "Epoch 125 of 200\n",
      "loss: 63.18\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0069, eta_b: 1.0073\n",
      "layer 2:\n",
      "eta_w: 1.0058, eta_b: 1.0063\n",
      "layer 3:\n",
      "eta_w: 1.0030, eta_b: 1.0022\n",
      "sigmoid_temp: 0.7187\n",
      "elapsed time: 47.0s\n",
      "\n",
      "Epoch 126 of 200\n",
      "loss: 63.15\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0074, eta_b: 1.0078\n",
      "layer 2:\n",
      "eta_w: 1.0065, eta_b: 1.0069\n",
      "layer 3:\n",
      "eta_w: 1.0040, eta_b: 1.0032\n",
      "sigmoid_temp: 0.7031\n",
      "elapsed time: 46.8s\n",
      "\n",
      "Epoch 127 of 200\n",
      "loss: 63.08\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0065, eta_b: 1.0071\n",
      "layer 2:\n",
      "eta_w: 1.0051, eta_b: 1.0057\n",
      "layer 3:\n",
      "eta_w: 1.0028, eta_b: 1.0019\n",
      "sigmoid_temp: 0.6975\n",
      "elapsed time: 46.5s\n",
      "\n",
      "Epoch 128 of 200\n",
      "loss: 63.02\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0061, eta_b: 1.0069\n",
      "layer 2:\n",
      "eta_w: 1.0048, eta_b: 1.0054\n",
      "layer 3:\n",
      "eta_w: 1.0029, eta_b: 1.0022\n",
      "sigmoid_temp: 0.7055\n",
      "elapsed time: 45.9s\n",
      "\n",
      "Epoch 129 of 200\n",
      "loss: 62.96\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0072, eta_b: 1.0077\n",
      "layer 2:\n",
      "eta_w: 1.0069, eta_b: 1.0075\n",
      "layer 3:\n",
      "eta_w: 1.0043, eta_b: 1.0036\n",
      "sigmoid_temp: 0.7938\n",
      "elapsed time: 47.2s\n",
      "\n",
      "Epoch 130 of 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 62.93\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0069, eta_b: 1.0071\n",
      "layer 2:\n",
      "eta_w: 1.0068, eta_b: 1.0070\n",
      "layer 3:\n",
      "eta_w: 1.0051, eta_b: 1.0045\n",
      "sigmoid_temp: 0.7366\n",
      "elapsed time: 46.0s\n",
      "\n",
      "Epoch 131 of 200\n",
      "loss: 62.87\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0063, eta_b: 1.0080\n",
      "layer 2:\n",
      "eta_w: 1.0063, eta_b: 1.0071\n",
      "layer 3:\n",
      "eta_w: 1.0037, eta_b: 1.0031\n",
      "sigmoid_temp: 0.6814\n",
      "elapsed time: 45.5s\n",
      "\n",
      "Epoch 132 of 200\n",
      "loss: 62.82\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0075, eta_b: 1.0092\n",
      "layer 2:\n",
      "eta_w: 1.0065, eta_b: 1.0073\n",
      "layer 3:\n",
      "eta_w: 1.0036, eta_b: 1.0032\n",
      "sigmoid_temp: 0.7661\n",
      "elapsed time: 46.2s\n",
      "\n",
      "Epoch 133 of 200\n",
      "loss: 62.77\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0060, eta_b: 1.0068\n",
      "layer 2:\n",
      "eta_w: 1.0063, eta_b: 1.0069\n",
      "layer 3:\n",
      "eta_w: 1.0044, eta_b: 1.0038\n",
      "sigmoid_temp: 0.7440\n",
      "elapsed time: 45.7s\n",
      "\n",
      "Epoch 134 of 200\n",
      "loss: 62.72\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0066, eta_b: 1.0088\n",
      "layer 2:\n",
      "eta_w: 1.0067, eta_b: 1.0076\n",
      "layer 3:\n",
      "eta_w: 1.0037, eta_b: 1.0033\n",
      "sigmoid_temp: 0.7028\n",
      "elapsed time: 45.9s\n",
      "\n",
      "Epoch 135 of 200\n",
      "loss: 62.66\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0057, eta_b: 1.0074\n",
      "layer 2:\n",
      "eta_w: 1.0050, eta_b: 1.0058\n",
      "layer 3:\n",
      "eta_w: 1.0034, eta_b: 1.0027\n",
      "sigmoid_temp: 0.6983\n",
      "elapsed time: 45.1s\n",
      "\n",
      "Epoch 136 of 200\n",
      "loss: 62.61\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0057, eta_b: 1.0066\n",
      "layer 2:\n",
      "eta_w: 1.0051, eta_b: 1.0054\n",
      "layer 3:\n",
      "eta_w: 1.0035, eta_b: 1.0028\n",
      "sigmoid_temp: 0.7625\n",
      "elapsed time: 44.5s\n",
      "\n",
      "Epoch 137 of 200\n",
      "loss: 62.57\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0040, eta_b: 1.0052\n",
      "layer 2:\n",
      "eta_w: 1.0043, eta_b: 1.0049\n",
      "layer 3:\n",
      "eta_w: 1.0024, eta_b: 1.0020\n",
      "sigmoid_temp: 0.7426\n",
      "elapsed time: 43.8s\n",
      "\n",
      "Epoch 138 of 200\n",
      "loss: 62.53\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0065, eta_b: 1.0074\n",
      "layer 2:\n",
      "eta_w: 1.0060, eta_b: 1.0062\n",
      "layer 3:\n",
      "eta_w: 1.0040, eta_b: 1.0036\n",
      "sigmoid_temp: 0.7504\n",
      "elapsed time: 44.1s\n",
      "\n",
      "Epoch 139 of 200\n",
      "loss: 62.49\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0051, eta_b: 1.0064\n",
      "layer 2:\n",
      "eta_w: 1.0058, eta_b: 1.0064\n",
      "layer 3:\n",
      "eta_w: 1.0043, eta_b: 1.0042\n",
      "sigmoid_temp: 0.7468\n",
      "elapsed time: 44.2s\n",
      "\n",
      "Epoch 140 of 200\n",
      "loss: 62.45\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0053, eta_b: 1.0061\n",
      "layer 2:\n",
      "eta_w: 1.0057, eta_b: 1.0061\n",
      "layer 3:\n",
      "eta_w: 1.0040, eta_b: 1.0040\n",
      "sigmoid_temp: 0.6781\n",
      "elapsed time: 46.1s\n",
      "\n",
      "Epoch 141 of 200\n",
      "loss: 62.39\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0058, eta_b: 1.0061\n",
      "layer 2:\n",
      "eta_w: 1.0057, eta_b: 1.0060\n",
      "layer 3:\n",
      "eta_w: 1.0042, eta_b: 1.0039\n",
      "sigmoid_temp: 0.6917\n",
      "elapsed time: 44.7s\n",
      "\n",
      "Epoch 142 of 200\n",
      "loss: 62.34\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0050, eta_b: 1.0064\n",
      "layer 2:\n",
      "eta_w: 1.0056, eta_b: 1.0061\n",
      "layer 3:\n",
      "eta_w: 1.0040, eta_b: 1.0036\n",
      "sigmoid_temp: 0.7324\n",
      "elapsed time: 45.0s\n",
      "\n",
      "Epoch 143 of 200\n",
      "loss: 62.31\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0060, eta_b: 1.0063\n",
      "layer 2:\n",
      "eta_w: 1.0062, eta_b: 1.0066\n",
      "layer 3:\n",
      "eta_w: 1.0044, eta_b: 1.0041\n",
      "sigmoid_temp: 0.7880\n",
      "elapsed time: 44.0s\n",
      "\n",
      "Epoch 144 of 200\n",
      "loss: 62.27\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0052, eta_b: 1.0066\n",
      "layer 2:\n",
      "eta_w: 1.0057, eta_b: 1.0062\n",
      "layer 3:\n",
      "eta_w: 1.0048, eta_b: 1.0044\n",
      "sigmoid_temp: 0.7643\n",
      "elapsed time: 44.0s\n",
      "\n",
      "Epoch 145 of 200\n",
      "loss: 62.24\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0044, eta_b: 1.0051\n",
      "layer 2:\n",
      "eta_w: 1.0040, eta_b: 1.0043\n",
      "layer 3:\n",
      "eta_w: 1.0035, eta_b: 1.0031\n",
      "sigmoid_temp: 0.6790\n",
      "elapsed time: 45.1s\n",
      "\n",
      "Epoch 146 of 200\n",
      "loss: 62.17\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0035, eta_b: 1.0047\n",
      "layer 2:\n",
      "eta_w: 1.0045, eta_b: 1.0049\n",
      "layer 3:\n",
      "eta_w: 1.0038, eta_b: 1.0033\n",
      "sigmoid_temp: 0.7759\n",
      "elapsed time: 44.5s\n",
      "\n",
      "Epoch 147 of 200\n",
      "loss: 62.14\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0023, eta_b: 1.0030\n",
      "layer 2:\n",
      "eta_w: 1.0027, eta_b: 1.0026\n",
      "layer 3:\n",
      "eta_w: 1.0030, eta_b: 1.0026\n",
      "sigmoid_temp: 0.7732\n",
      "elapsed time: 45.2s\n",
      "\n",
      "Epoch 148 of 200\n",
      "loss: 62.10\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0054, eta_b: 1.0066\n",
      "layer 2:\n",
      "eta_w: 1.0055, eta_b: 1.0060\n",
      "layer 3:\n",
      "eta_w: 1.0048, eta_b: 1.0046\n",
      "sigmoid_temp: 0.7628\n",
      "elapsed time: 44.7s\n",
      "\n",
      "Epoch 149 of 200\n",
      "loss: 62.07\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0053, eta_b: 1.0049\n",
      "layer 2:\n",
      "eta_w: 1.0056, eta_b: 1.0055\n",
      "layer 3:\n",
      "eta_w: 1.0055, eta_b: 1.0057\n",
      "sigmoid_temp: 0.7331\n",
      "elapsed time: 45.1s\n",
      "\n",
      "Epoch 150 of 200\n",
      "loss: 62.02\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0064, eta_b: 1.0065\n",
      "layer 2:\n",
      "eta_w: 1.0061, eta_b: 1.0062\n",
      "layer 3:\n",
      "eta_w: 1.0057, eta_b: 1.0058\n",
      "sigmoid_temp: 0.7797\n",
      "elapsed time: 44.7s\n",
      "\n",
      "Epoch 151 of 200\n",
      "loss: 61.99\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0045, eta_b: 1.0042\n",
      "layer 2:\n",
      "eta_w: 1.0049, eta_b: 1.0050\n",
      "layer 3:\n",
      "eta_w: 1.0054, eta_b: 1.0058\n",
      "sigmoid_temp: 0.7798\n",
      "elapsed time: 44.8s\n",
      "\n",
      "Epoch 152 of 200\n",
      "loss: 61.95\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0040, eta_b: 1.0047\n",
      "layer 2:\n",
      "eta_w: 1.0044, eta_b: 1.0047\n",
      "layer 3:\n",
      "eta_w: 1.0044, eta_b: 1.0046\n",
      "sigmoid_temp: 0.7155\n",
      "elapsed time: 45.3s\n",
      "\n",
      "Epoch 153 of 200\n",
      "loss: 61.91\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0030, eta_b: 1.0050\n",
      "layer 2:\n",
      "eta_w: 1.0039, eta_b: 1.0043\n",
      "layer 3:\n",
      "eta_w: 1.0047, eta_b: 1.0047\n",
      "sigmoid_temp: 0.7236\n",
      "elapsed time: 45.2s\n",
      "\n",
      "Epoch 154 of 200\n",
      "loss: 61.86\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0035, eta_b: 1.0049\n",
      "layer 2:\n",
      "eta_w: 1.0045, eta_b: 1.0050\n",
      "layer 3:\n",
      "eta_w: 1.0049, eta_b: 1.0048\n",
      "sigmoid_temp: 0.7565\n",
      "elapsed time: 46.1s\n",
      "\n",
      "Epoch 155 of 200\n",
      "loss: 61.84\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0035, eta_b: 1.0050\n",
      "layer 2:\n",
      "eta_w: 1.0037, eta_b: 1.0041\n",
      "layer 3:\n",
      "eta_w: 1.0044, eta_b: 1.0044\n",
      "sigmoid_temp: 0.7554\n",
      "elapsed time: 46.7s\n",
      "\n",
      "Epoch 156 of 200\n",
      "loss: 61.81\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0006, eta_b: 1.0029\n",
      "layer 2:\n",
      "eta_w: 1.0011, eta_b: 1.0014\n",
      "layer 3:\n",
      "eta_w: 1.0027, eta_b: 1.0022\n",
      "sigmoid_temp: 0.7657\n",
      "elapsed time: 46.8s\n",
      "\n",
      "Epoch 157 of 200\n",
      "loss: 61.77\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0037, eta_b: 1.0051\n",
      "layer 2:\n",
      "eta_w: 1.0050, eta_b: 1.0055\n",
      "layer 3:\n",
      "eta_w: 1.0050, eta_b: 1.0048\n",
      "sigmoid_temp: 0.7558\n",
      "elapsed time: 46.2s\n",
      "\n",
      "Epoch 158 of 200\n",
      "loss: 61.75\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0031, eta_b: 1.0040\n",
      "layer 2:\n",
      "eta_w: 1.0041, eta_b: 1.0041\n",
      "layer 3:\n",
      "eta_w: 1.0052, eta_b: 1.0049\n",
      "sigmoid_temp: 0.6984\n",
      "elapsed time: 45.8s\n",
      "\n",
      "Epoch 159 of 200\n",
      "loss: 61.70\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0016, eta_b: 1.0025\n",
      "layer 2:\n",
      "eta_w: 1.0021, eta_b: 1.0023\n",
      "layer 3:\n",
      "eta_w: 1.0031, eta_b: 1.0028\n",
      "sigmoid_temp: 0.5966\n",
      "elapsed time: 46.3s\n",
      "\n",
      "Epoch 160 of 200\n",
      "loss: 61.62\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0018, eta_b: 1.0024\n",
      "layer 2:\n",
      "eta_w: 1.0028, eta_b: 1.0027\n",
      "layer 3:\n",
      "eta_w: 1.0041, eta_b: 1.0034\n",
      "sigmoid_temp: 0.7171\n",
      "elapsed time: 45.1s\n",
      "\n",
      "Epoch 161 of 200\n",
      "loss: 61.60\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0027, eta_b: 1.0032\n",
      "layer 2:\n",
      "eta_w: 1.0032, eta_b: 1.0030\n",
      "layer 3:\n",
      "eta_w: 1.0038, eta_b: 1.0034\n",
      "sigmoid_temp: 0.6322\n",
      "elapsed time: 46.9s\n",
      "\n",
      "Epoch 162 of 200\n",
      "loss: 61.54\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0033, eta_b: 1.0040\n",
      "layer 2:\n",
      "eta_w: 1.0037, eta_b: 1.0038\n",
      "layer 3:\n",
      "eta_w: 1.0035, eta_b: 1.0029\n",
      "sigmoid_temp: 0.7044\n",
      "elapsed time: 46.7s\n",
      "\n",
      "Epoch 163 of 200\n",
      "loss: 61.51\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0030, eta_b: 1.0031\n",
      "layer 2:\n",
      "eta_w: 1.0025, eta_b: 1.0022\n",
      "layer 3:\n",
      "eta_w: 1.0035, eta_b: 1.0029\n",
      "sigmoid_temp: 0.7932\n",
      "elapsed time: 46.5s\n",
      "\n",
      "Epoch 164 of 200\n",
      "loss: 61.48\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0025, eta_b: 1.0022\n",
      "layer 2:\n",
      "eta_w: 1.0027, eta_b: 1.0025\n",
      "layer 3:\n",
      "eta_w: 1.0036, eta_b: 1.0032\n",
      "sigmoid_temp: 0.7809\n",
      "elapsed time: 45.8s\n",
      "\n",
      "Epoch 165 of 200\n",
      "loss: 61.46\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0044, eta_b: 1.0048\n",
      "layer 2:\n",
      "eta_w: 1.0044, eta_b: 1.0045\n",
      "layer 3:\n",
      "eta_w: 1.0043, eta_b: 1.0039\n",
      "sigmoid_temp: 0.7754\n",
      "elapsed time: 45.5s\n",
      "\n",
      "Epoch 166 of 200\n",
      "loss: 61.43\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0016, eta_b: 1.0023\n",
      "layer 2:\n",
      "eta_w: 1.0020, eta_b: 1.0019\n",
      "layer 3:\n",
      "eta_w: 1.0022, eta_b: 1.0016\n",
      "sigmoid_temp: 0.7326\n",
      "elapsed time: 45.5s\n",
      "\n",
      "Epoch 167 of 200\n",
      "loss: 61.40\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0028, eta_b: 1.0032\n",
      "layer 2:\n",
      "eta_w: 1.0025, eta_b: 1.0024\n",
      "layer 3:\n",
      "eta_w: 1.0017, eta_b: 1.0015\n",
      "sigmoid_temp: 0.7704\n",
      "elapsed time: 45.8s\n",
      "\n",
      "Epoch 168 of 200\n",
      "loss: 61.37\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0001, eta_b: 1.0010\n",
      "layer 2:\n",
      "eta_w: 1.0004, eta_b: 1.0002\n",
      "layer 3:\n",
      "eta_w: 1.0017, eta_b: 1.0012\n",
      "sigmoid_temp: 0.7158\n",
      "elapsed time: 45.8s\n",
      "\n",
      "Epoch 169 of 200\n",
      "loss: 61.33\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0039, eta_b: 1.0051\n",
      "layer 2:\n",
      "eta_w: 1.0034, eta_b: 1.0036\n",
      "layer 3:\n",
      "eta_w: 1.0022, eta_b: 1.0019\n",
      "sigmoid_temp: 0.8034\n",
      "elapsed time: 46.4s\n",
      "\n",
      "Epoch 170 of 200\n",
      "loss: 61.31\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0026, eta_b: 1.0036\n",
      "layer 2:\n",
      "eta_w: 1.0028, eta_b: 1.0028\n",
      "layer 3:\n",
      "eta_w: 1.0023, eta_b: 1.0018\n",
      "sigmoid_temp: 0.7642\n",
      "elapsed time: 46.0s\n",
      "\n",
      "Epoch 171 of 200\n",
      "loss: 61.29\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0070, eta_b: 1.0074\n",
      "layer 2:\n",
      "eta_w: 1.0069, eta_b: 1.0068\n",
      "layer 3:\n",
      "eta_w: 1.0061, eta_b: 1.0060\n",
      "sigmoid_temp: 0.8832\n",
      "elapsed time: 46.1s\n",
      "\n",
      "Epoch 172 of 200\n",
      "loss: 61.27\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0023, eta_b: 1.0028\n",
      "layer 2:\n",
      "eta_w: 1.0025, eta_b: 1.0024\n",
      "layer 3:\n",
      "eta_w: 1.0021, eta_b: 1.0016\n",
      "sigmoid_temp: 0.7037\n",
      "elapsed time: 45.6s\n",
      "\n",
      "Epoch 173 of 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 61.22\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0018, eta_b: 1.0016\n",
      "layer 2:\n",
      "eta_w: 1.0029, eta_b: 1.0027\n",
      "layer 3:\n",
      "eta_w: 1.0025, eta_b: 1.0026\n",
      "sigmoid_temp: 0.7137\n",
      "elapsed time: 45.9s\n",
      "\n",
      "Epoch 174 of 200\n",
      "loss: 61.18\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0038, eta_b: 1.0042\n",
      "layer 2:\n",
      "eta_w: 1.0039, eta_b: 1.0039\n",
      "layer 3:\n",
      "eta_w: 1.0026, eta_b: 1.0026\n",
      "sigmoid_temp: 0.7605\n",
      "elapsed time: 45.6s\n",
      "\n",
      "Epoch 175 of 200\n",
      "loss: 61.15\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0052, eta_b: 1.0045\n",
      "layer 2:\n",
      "eta_w: 1.0051, eta_b: 1.0052\n",
      "layer 3:\n",
      "eta_w: 1.0027, eta_b: 1.0026\n",
      "sigmoid_temp: 0.7623\n",
      "elapsed time: 45.6s\n",
      "\n",
      "Epoch 176 of 200\n",
      "loss: 61.12\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0070, eta_b: 1.0078\n",
      "layer 2:\n",
      "eta_w: 1.0061, eta_b: 1.0062\n",
      "layer 3:\n",
      "eta_w: 1.0050, eta_b: 1.0049\n",
      "sigmoid_temp: 0.8116\n",
      "elapsed time: 44.9s\n",
      "\n",
      "Epoch 177 of 200\n",
      "loss: 61.10\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0020, eta_b: 1.0023\n",
      "layer 2:\n",
      "eta_w: 1.0019, eta_b: 1.0018\n",
      "layer 3:\n",
      "eta_w: 1.0004, eta_b: 1.0005\n",
      "sigmoid_temp: 0.7068\n",
      "elapsed time: 44.9s\n",
      "\n",
      "Epoch 178 of 200\n",
      "loss: 61.05\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0024, eta_b: 1.0022\n",
      "layer 2:\n",
      "eta_w: 1.0019, eta_b: 1.0021\n",
      "layer 3:\n",
      "eta_w: 1.0008, eta_b: 1.0010\n",
      "sigmoid_temp: 0.7776\n",
      "elapsed time: 44.5s\n",
      "\n",
      "Epoch 179 of 200\n",
      "loss: 61.02\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0032, eta_b: 1.0033\n",
      "layer 2:\n",
      "eta_w: 1.0026, eta_b: 1.0026\n",
      "layer 3:\n",
      "eta_w: 1.0020, eta_b: 1.0026\n",
      "sigmoid_temp: 0.8060\n",
      "elapsed time: 45.0s\n",
      "\n",
      "Epoch 180 of 200\n",
      "loss: 61.02\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0020, eta_b: 1.0029\n",
      "layer 2:\n",
      "eta_w: 1.0021, eta_b: 1.0022\n",
      "layer 3:\n",
      "eta_w: 1.0018, eta_b: 1.0019\n",
      "sigmoid_temp: 0.7244\n",
      "elapsed time: 43.7s\n",
      "\n",
      "Epoch 181 of 200\n",
      "loss: 60.96\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0023, eta_b: 1.0019\n",
      "layer 2:\n",
      "eta_w: 1.0021, eta_b: 1.0016\n",
      "layer 3:\n",
      "eta_w: 1.0010, eta_b: 1.0011\n",
      "sigmoid_temp: 0.6986\n",
      "elapsed time: 44.8s\n",
      "\n",
      "Epoch 182 of 200\n",
      "loss: 60.93\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 0.9992, eta_b: 0.9994\n",
      "layer 2:\n",
      "eta_w: 1.0002, eta_b: 1.0001\n",
      "layer 3:\n",
      "eta_w: 1.0002, eta_b: 1.0005\n",
      "sigmoid_temp: 0.7296\n",
      "elapsed time: 45.2s\n",
      "\n",
      "Epoch 183 of 200\n",
      "loss: 60.90\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0025, eta_b: 1.0026\n",
      "layer 2:\n",
      "eta_w: 1.0026, eta_b: 1.0025\n",
      "layer 3:\n",
      "eta_w: 1.0014, eta_b: 1.0014\n",
      "sigmoid_temp: 0.7028\n",
      "elapsed time: 45.1s\n",
      "\n",
      "Epoch 184 of 200\n",
      "loss: 60.86\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0005, eta_b: 1.0012\n",
      "layer 2:\n",
      "eta_w: 1.0012, eta_b: 1.0015\n",
      "layer 3:\n",
      "eta_w: 0.9997, eta_b: 1.0001\n",
      "sigmoid_temp: 0.7234\n",
      "elapsed time: 45.6s\n",
      "\n",
      "Epoch 185 of 200\n",
      "loss: 60.83\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0025, eta_b: 1.0021\n",
      "layer 2:\n",
      "eta_w: 1.0027, eta_b: 1.0025\n",
      "layer 3:\n",
      "eta_w: 1.0020, eta_b: 1.0025\n",
      "sigmoid_temp: 0.7952\n",
      "elapsed time: 45.0s\n",
      "\n",
      "Epoch 186 of 200\n",
      "loss: 60.83\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0048, eta_b: 1.0048\n",
      "layer 2:\n",
      "eta_w: 1.0042, eta_b: 1.0042\n",
      "layer 3:\n",
      "eta_w: 1.0021, eta_b: 1.0020\n",
      "sigmoid_temp: 0.7939\n",
      "elapsed time: 45.4s\n",
      "\n",
      "Epoch 187 of 200\n",
      "loss: 60.80\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0045, eta_b: 1.0049\n",
      "layer 2:\n",
      "eta_w: 1.0041, eta_b: 1.0042\n",
      "layer 3:\n",
      "eta_w: 1.0029, eta_b: 1.0030\n",
      "sigmoid_temp: 0.8153\n",
      "elapsed time: 45.9s\n",
      "\n",
      "Epoch 188 of 200\n",
      "loss: 60.79\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0013, eta_b: 1.0016\n",
      "layer 2:\n",
      "eta_w: 1.0018, eta_b: 1.0018\n",
      "layer 3:\n",
      "eta_w: 1.0011, eta_b: 1.0013\n",
      "sigmoid_temp: 0.7818\n",
      "elapsed time: 45.9s\n",
      "\n",
      "Epoch 189 of 200\n",
      "loss: 60.76\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0031, eta_b: 1.0030\n",
      "layer 2:\n",
      "eta_w: 1.0028, eta_b: 1.0028\n",
      "layer 3:\n",
      "eta_w: 1.0010, eta_b: 1.0013\n",
      "sigmoid_temp: 0.7758\n",
      "elapsed time: 46.0s\n",
      "\n",
      "Epoch 190 of 200\n",
      "loss: 60.73\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0031, eta_b: 1.0027\n",
      "layer 2:\n",
      "eta_w: 1.0032, eta_b: 1.0031\n",
      "layer 3:\n",
      "eta_w: 1.0015, eta_b: 1.0018\n",
      "sigmoid_temp: 0.8067\n",
      "elapsed time: 47.1s\n",
      "\n",
      "Epoch 191 of 200\n",
      "loss: 60.71\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0035, eta_b: 1.0026\n",
      "layer 2:\n",
      "eta_w: 1.0030, eta_b: 1.0026\n",
      "layer 3:\n",
      "eta_w: 1.0010, eta_b: 1.0007\n",
      "sigmoid_temp: 0.7441\n",
      "elapsed time: 46.3s\n",
      "\n",
      "Epoch 192 of 200\n",
      "loss: 60.68\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0055, eta_b: 1.0065\n",
      "layer 2:\n",
      "eta_w: 1.0046, eta_b: 1.0046\n",
      "layer 3:\n",
      "eta_w: 1.0012, eta_b: 1.0013\n",
      "sigmoid_temp: 0.8259\n",
      "elapsed time: 46.4s\n",
      "\n",
      "Epoch 193 of 200\n",
      "loss: 60.66\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0029, eta_b: 1.0033\n",
      "layer 2:\n",
      "eta_w: 1.0038, eta_b: 1.0038\n",
      "layer 3:\n",
      "eta_w: 1.0019, eta_b: 1.0023\n",
      "sigmoid_temp: 0.7946\n",
      "elapsed time: 47.0s\n",
      "\n",
      "Epoch 194 of 200\n",
      "loss: 60.64\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0031, eta_b: 1.0041\n",
      "layer 2:\n",
      "eta_w: 1.0030, eta_b: 1.0033\n",
      "layer 3:\n",
      "eta_w: 1.0010, eta_b: 1.0013\n",
      "sigmoid_temp: 0.7593\n",
      "elapsed time: 46.8s\n",
      "\n",
      "Epoch 195 of 200\n",
      "loss: 60.61\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0011, eta_b: 1.0022\n",
      "layer 2:\n",
      "eta_w: 1.0004, eta_b: 1.0003\n",
      "layer 3:\n",
      "eta_w: 1.0003, eta_b: 1.0001\n",
      "sigmoid_temp: 0.6790\n",
      "elapsed time: 46.9s\n",
      "\n",
      "Epoch 196 of 200\n",
      "loss: 60.57\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0041, eta_b: 1.0040\n",
      "layer 2:\n",
      "eta_w: 1.0034, eta_b: 1.0030\n",
      "layer 3:\n",
      "eta_w: 1.0031, eta_b: 1.0030\n",
      "sigmoid_temp: 0.7939\n",
      "elapsed time: 46.9s\n",
      "\n",
      "Epoch 197 of 200\n",
      "loss: 60.55\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0007, eta_b: 1.0025\n",
      "layer 2:\n",
      "eta_w: 1.0018, eta_b: 1.0021\n",
      "layer 3:\n",
      "eta_w: 1.0013, eta_b: 1.0012\n",
      "sigmoid_temp: 0.7082\n",
      "elapsed time: 51.8s\n",
      "\n",
      "Epoch 198 of 200\n",
      "loss: 60.51\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0019, eta_b: 1.0027\n",
      "layer 2:\n",
      "eta_w: 1.0021, eta_b: 1.0020\n",
      "layer 3:\n",
      "eta_w: 1.0024, eta_b: 1.0023\n",
      "sigmoid_temp: 0.7730\n",
      "elapsed time: 60.5s\n",
      "\n",
      "Epoch 199 of 200\n",
      "loss: 60.49\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0024, eta_b: 1.0047\n",
      "layer 2:\n",
      "eta_w: 1.0022, eta_b: 1.0025\n",
      "layer 3:\n",
      "eta_w: 1.0008, eta_b: 1.0005\n",
      "sigmoid_temp: 0.7710\n",
      "elapsed time: 60.4s\n",
      "\n",
      "Epoch 200 of 200\n",
      "loss: 60.48\n",
      "block 1:\n",
      "layer 1:\n",
      "eta_w: 1.0031, eta_b: 1.0048\n",
      "layer 2:\n",
      "eta_w: 1.0019, eta_b: 1.0021\n",
      "layer 3:\n",
      "eta_w: 1.0007, eta_b: 1.0006\n",
      "sigmoid_temp: 0.7743\n",
      "elapsed time: 59.6s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "num_steps = 500 * 1000\n",
    "num_epochs = num_steps * batch_size // train_loader.dataset.train_data.size(0)\n",
    "#num_epochs = 10\n",
    "num_blocks = 1\n",
    "\n",
    "#opt_modes = ['last_epoch_reward_baseline', 'no_baseline', 'reparam_1.0', 'reparam_0.1', 'reparam_10.']\n",
    "opt_modes = ['rebar_0.1']\n",
    "train_losses = {}\n",
    "\n",
    "learning_rate = 0.0001\n",
    "eta_start = 1.0\n",
    "\n",
    "logsigmoid = nn.LogSigmoid()\n",
    "\n",
    "for opt_mode in opt_modes:\n",
    "    print()\n",
    "    print(opt_mode)\n",
    "    train_losses[opt_mode] = []\n",
    "    model = cuda_wrapper(SigmoidBeliefNetwork(input_dim, num_blocks=num_blocks, hidden_dim=200, nonlinear_blocks=True))\n",
    "    if num_blocks > 0:\n",
    "        num_layers = len(model.dense_layers[0])\n",
    "    else:\n",
    "        num_layers = 0\n",
    "        \n",
    "    eta = autograd.Variable(\n",
    "        cuda_wrapper(torch.Tensor(num_blocks, num_layers, 2).fill_(eta_start)), requires_grad=True\n",
    "    )\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.99999))\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time()\n",
    "        print('Epoch {} of {}'.format(epoch+1, num_epochs))\n",
    "        \n",
    "        torch.manual_seed(42)\n",
    "        \n",
    "        if opt_mode.startswith('reparam'):\n",
    "            sigmoid_temp = float(opt_mode.split('_')[1])\n",
    "        elif opt_mode.startswith('rebar'):\n",
    "            sigmoid_temp = autograd.Variable(\n",
    "                cuda_wrapper(torch.Tensor([float(opt_mode.split('_')[1])])), requires_grad=True\n",
    "            )\n",
    "            optimizer_variance = optim.Adam([eta, sigmoid_temp], betas=(0.9, 0.99999))\n",
    "        else:\n",
    "            sigmoid_temp = 1.0\n",
    "        \n",
    "        loader = train_loader\n",
    "        num_samples = train_loader.dataset.train_data.size(0)\n",
    "        \n",
    "        cum_losses = 0\n",
    "        for batch_idx, (data, target) in enumerate(loader):\n",
    "            top, bottom = torch.chunk(\n",
    "                (data.view(data.size(0), -1) > 0.5).float(), 2, dim=1\n",
    "            )\n",
    "            top, bottom = autograd.Variable(cuda_wrapper(top), requires_grad=False), \\\n",
    "                          autograd.Variable(cuda_wrapper(bottom), requires_grad=False)\n",
    "            hard_bottom_logit, soft_bottom_logits, soft_cond_bottom_logits = model(top, sigmoid_temp)\n",
    "\n",
    "            hard_samplewise_loss = -torch.sum(\n",
    "                logsigmoid(hard_bottom_logit) * bottom + logsigmoid(-hard_bottom_logit) * (1 - bottom), dim=1\n",
    "            )\n",
    "            hard_loss = torch.sum(hard_samplewise_loss)\n",
    "\n",
    "            soft_samplewise_losses = []\n",
    "            soft_cond_samplewise_losses = []\n",
    "            soft_losses = []\n",
    "            soft_cond_losses = []\n",
    "            for i in range(num_blocks):\n",
    "                soft_samplewise_losses.append(\n",
    "                    -torch.sum(\n",
    "                        logsigmoid(soft_bottom_logits[i]) * bottom +\\\n",
    "                        logsigmoid(-soft_bottom_logits[i]) * (1 - bottom), dim=1\n",
    "                    )\n",
    "                )\n",
    "                soft_losses.append(torch.sum(soft_samplewise_losses[-1]))\n",
    "                \n",
    "                soft_cond_samplewise_losses.append(\n",
    "                    -torch.sum\n",
    "                    (\n",
    "                        logsigmoid(soft_cond_bottom_logits[i]) * bottom +\\\n",
    "                        logsigmoid(-soft_cond_bottom_logits[i]) * (1 - bottom), dim=1\n",
    "                    )\n",
    "                )\n",
    "                soft_cond_losses.append(torch.sum(soft_cond_samplewise_losses[-1]))\n",
    "            \n",
    "            cum_losses += hard_loss.data.mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if opt_mode.startswith('reparam'):\n",
    "                soft_losses[0].backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "            elif opt_mode in ['no_baseline', 'last_epoch_reward_baseline']:\n",
    "                cost = torch.stack([hard_samplewise_loss.detach()] * model.hidden_dim, dim=1)\n",
    "                if opt_mode == 'no_baseline':\n",
    "                    baseline = 0\n",
    "                elif opt_mode == 'last_epoch_reward_baseline':\n",
    "                    baseline = autograd.Variable(train_losses[opt_mode][-1], requires_grad=False) if epoch > 0 else 0\n",
    "                for sample_log_prob in model.sample_log_probs:\n",
    "                    hard_loss += torch.sum(sample_log_prob * (cost - baseline))\n",
    "                hard_loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "            elif opt_mode.startswith('rebar'):\n",
    "                loss_eta_linear_coef_grads = []\n",
    "                for i in range(num_blocks):\n",
    "                    cost = torch.stack(\n",
    "                        [-soft_cond_samplewise_losses[i].detach()] * model.hidden_dim, dim=1\n",
    "                    )\n",
    "                    loss_eta_linear_coef = soft_losses[i] - soft_cond_losses[i]\n",
    "                    loss_eta_linear_coef += torch.sum(model.sample_log_probs[i] * cost)\n",
    "                    loss_eta_linear_coef_grads.append(\n",
    "                        autograd.grad(\n",
    "                            loss_eta_linear_coef,\n",
    "                            [param for dense_layer in model.dense_layers[i] for param in dense_layer.parameters()],\n",
    "                            create_graph=True, retain_graph=True\n",
    "                        )\n",
    "                    )\n",
    "                            \n",
    "                cost_additional = torch.stack(\n",
    "                    [hard_samplewise_loss.detach()] * model.hidden_dim, dim=1\n",
    "                )\n",
    "                loss_additional = 0\n",
    "                for sample_log_prob in model.sample_log_probs:\n",
    "                    loss_additional += torch.sum(sample_log_prob * cost_additional)\n",
    "                loss_additional_grads = autograd.grad(\n",
    "                    loss_additional,\n",
    "                    [\n",
    "                        param\n",
    "                        for i in range(num_blocks)\n",
    "                        for dense_layer in model.dense_layers[i] \n",
    "                        for param in dense_layer.parameters()\n",
    "                    ],\n",
    "                    create_graph=True, retain_graph=True\n",
    "                )\n",
    "                \n",
    "                hard_loss.backward(retain_graph=True)\n",
    "\n",
    "                loss_grads = []\n",
    "                for i in range(num_blocks):\n",
    "                    loss_grads.append([])\n",
    "                    for j, dense_layer in enumerate(model.dense_layers[i]):\n",
    "                        loss_grads[-1].append([])\n",
    "                        for k, param in enumerate(dense_layer.parameters()):\n",
    "                            loss_grads[-1][-1].append(\n",
    "                                eta[i, j, k] * loss_eta_linear_coef_grads[i][j * 2 + k] +\\\n",
    "                                loss_additional_grads[(i * num_layers + j) * 2 + k]\n",
    "                            )\n",
    "                            param.grad = loss_grads[-1][-1][-1]\n",
    "\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                variance = 0\n",
    "                for i in range(num_blocks):\n",
    "                    for j, dense_layer in enumerate(model.dense_layers[i]):\n",
    "                        for k, param in enumerate(dense_layer.parameters()):\n",
    "                            variance += torch.sum(loss_grads[i][j][k] ** 2)\n",
    "                \n",
    "                optimizer_variance.zero_grad()\n",
    "                variance.backward()\n",
    "                optimizer_variance.step()\n",
    "                \n",
    "        mean_loss = cum_losses / num_samples\n",
    "        print(\"loss: {:.2f}\".format(mean_loss))\n",
    "        train_losses[opt_mode].append(mean_loss)\n",
    "        \n",
    "        for i in range(num_blocks):\n",
    "            print('block {}:'.format(i + 1))\n",
    "            for j in range(num_layers):\n",
    "                print('layer {}:'.format(j + 1))\n",
    "                print('eta_w: {:.4f}, eta_b: {:.4f}'.format(eta.data[i, j, 0], eta.data[i, j, 1]))\n",
    "        if opt_mode.startswith('rebar'):\n",
    "            print('sigmoid_temp: {:.4f}'.format(sigmoid_temp.data[0]))\n",
    "        \n",
    "        print('elapsed time: {:.1f}s'.format(time() - start_time))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 layer non-linear model:\n",
    "\n",
    "##### 200 epochs corresponds to 500 thousands steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f92d91e20b8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3kAAAHVCAYAAABMsTJpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f92d91e20f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for opt_mode in opt_modes:\n",
    "    plt.plot(range(1, num_epochs+1), -np.array(train_losses[opt_mode]))\n",
    "plt.grid(True)\n",
    "plt.legend(opt_modes)\n",
    "#plt.plot(test_losses, 'k--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final performance:\n",
      "rebar_0.1: -60.48\n"
     ]
    }
   ],
   "source": [
    "print('Final performance:')\n",
    "for opt_mode in opt_modes:\n",
    "    print('{}: {:.2f}'.format(opt_mode, -train_losses[opt_mode][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
